{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinaMariaCastro/curso-ia-para-economia/blob/main/clases/5_Aprendizaje_supervisado/5_Ensamble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "DHNQuIqAmvl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inteligencia Artificial con Aplicaciones en Econom√≠a I**\n",
        "\n",
        "- üë©‚Äçüè´ **Profesora:** [Lina Mar√≠a Castro](https://www.linkedin.com/in/lina-maria-castro)  \n",
        "- üìß **Email:** [lmcastroco@gmail.com](mailto:lmcastroco@gmail.com)  \n",
        "- üéì **Universidad:** Universidad Externado de Colombia - Facultad de Econom√≠a"
      ],
      "metadata": {
        "id": "5iiwfbXCQv5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü•¶ü§ù **M√©todos de Ensamble: Random Forest y Gradient Boosting**"
      ],
      "metadata": {
        "id": "M8idSdAW8Odz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ofu0lNAXTlxF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmTL-jQwT8Ub"
      },
      "source": [
        "**Objetivos de Aprendizaje**\n",
        "\n",
        "Al finalizar este notebook, ser√°s capaz de:\n",
        "\n",
        "1.  **Explicar la intuici√≥n** detr√°s de los m√©todos de ensamble como Random Forest (Bagging) y Gradient Boosting (Boosting).\n",
        "2.  **Implementar y comparar** modelos de ensamble con un modelo base (√Årbol de Decisi√≥n) utilizando `scikit-learn`.\n",
        "3.  **Aplicar** el uso de m√©tricas avanzadas como la Curva ROC y el AUC en contextos con clases desbalanceadas, como el riesgo crediticio.\n",
        "4. **Demostrar** de forma r√°pida y directa c√≥mo cambia el rendimiento de cada modelo en el set de prueba.\n",
        "5.  **Extraer** la importancia de las variables (`feature importance`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFlFgMxOT8Ud"
      },
      "source": [
        "**Introducci√≥n: La Sabidur√≠a del Comit√©**\n",
        "\n",
        "Hasta ahora, hemos entrenado modelos de forma individual. Un √°rbol de decisi√≥n, una regresi√≥n log√≠stica, una regresi√≥n lineal, etc. Pero, ¬øqu√© pasar√≠a si en lugar de confiar en la opini√≥n de un solo \"experto\" (un modelo), creamos un comit√© de expertos y basamos nuestra decisi√≥n en su sabidur√≠a colectiva?\n",
        "\n",
        "Esta es la idea central detr√°s de los **m√©todos de ensamble**. Combinan m√∫ltiples modelos, a menudo llamados \"aprendices d√©biles\", para crear un \"aprendiz fuerte\" mucho m√°s robusto, preciso y menos propenso al sobreajuste.\n",
        "\n",
        "**Analog√≠a**\n",
        "\n",
        "* **Random Forest (Bagging):** Imagina un **comit√© de inversi√≥n diverso**. En lugar de confiar en un solo analista estrella (un √∫nico √°rbol de decisi√≥n), el portafolio se beneficia de las opiniones diversas y no correlacionadas de muchos analistas. Cada analista recibe una muestra ligeramente diferente de la informaci√≥n disponible. Incluso si algunos se equivocan, el **promedio del comit√© es mucho m√°s estable y acertado**.\n",
        "\n",
        "* **Gradient Boosting (Boosting):** Piensa en un **programa de mentor√≠a para analistas de riesgo**. El primer analista (modelo 1) eval√∫a una solicitud de cr√©dito y comete algunos errores. Un analista senior o mentor (modelo 2) revisa esos errores y aprende a identificarlos. Luego, un tercer analista (modelo 3) se enfoca en los errores que cometieron los dos primeros. Este proceso secuencial de **aprender de los errores anteriores** crea un pron√≥stico final muy refinado y preciso."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE:**\n",
        "\n",
        "- Ambos resuelven tanto problemas de regresi√≥n como de clasificaci√≥n (vamos a ver un ejemplo de clasificaci√≥n y en el taller van a trabajar un ejemplo de regresi√≥n)."
      ],
      "metadata": {
        "id": "qdxXkqdwsDVy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zd3bJ1GT8Uf"
      },
      "source": [
        "## 1. Preparaci√≥n del Entorno y Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAEXrJ56T8Ug"
      },
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as est√°ndar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# M√≥dulos de Scikit-Learn para modelado y evaluaci√≥n\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mejorar visualizaci√≥n de dataframes y gr√°ficos"
      ],
      "metadata": {
        "id": "jHE2ZLJuEPIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Que muestre todas las columnas\n",
        "pd.options.display.max_columns = None\n",
        "# En los dataframes, mostrar los float con dos decimales\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "# Configuraciones para una mejor visualizaci√≥n\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "id": "72TA8V1fETCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccSy_DV7T8Ug"
      },
      "source": [
        "### Carga y Limpieza de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el dataset **German Credit Data** (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data), el cual contiene informaci√≥n sobre 1000 solicitantes de cr√©dito. El conjunto de datos incluye a individuos a quienes efectivamente se les otorg√≥ un cr√©dito. No contiene informaci√≥n sobre los solicitantes que fueron rechazados.\n",
        "\n",
        "Para cada uno de estos 1000 individuos, el banco ya realiz√≥ una clasificaci√≥n de riesgo a posteriori, es decir, despu√©s de observar su comportamiento de pago. **La columna objetivo es Risk (o 'Riesgo')** indica si, con el tiempo, el cliente result√≥ ser un buen pagador (cumpli√≥ con sus obligaciones) o un mal pagador (incurri√≥ en impago o default).\n",
        "\n",
        "Por lo tanto, el objetivo de un modelo entrenado con estos datos no es decidir si se aprueba o no un cr√©dito (ya que todos fueron aprobados), sino predecir, en el momento de la solicitud, la probabilidad de que un cliente aprobado se convierta en un mal pagador en el futuro."
      ],
      "metadata": {
        "id": "-v-mABKOAgKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset original de la UCI no tiene nombres de columna y las variables categ√≥ricas necesitan ser procesadas."
      ],
      "metadata": {
        "id": "isD2tadSAQwW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A13oyxd4UkCL"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset desde una URL p√∫blica\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "# Los nombres de las columnas se encuentran en la documentaci√≥n del dataset\n",
        "columns = ['Estado de la cuenta corriente existente', 'Duraci√≥n en meses', 'Historia de cr√©dito', 'Prop√≥sito',\n",
        "           'Monto del cr√©dito',\n",
        "           'Cuenta de ahorros/bonos', 'Empleo actual desde',\n",
        "           'Tasa de pago a plazos en porcentaje del ingreso disponible',\n",
        "           'Estado personal y sexo', 'Otros deudores/fiadores', 'Residencia actual desde', 'Propiedad',\n",
        "           'Edad', 'Otros planes de pago', 'Alojamiento', 'N√∫mero de cr√©ditos existentes en este banco',\n",
        "           'Trabajo', 'N√∫mero de personas obligadas a prestar manutenci√≥n a', 'Tel√©fono', 'Trabajador extranjero',\n",
        "           'Riesgo']\n",
        "\n",
        "df = pd.read_csv(url, sep=' ', header=None, names=columns)\n",
        "\n",
        "# Vistazo inicial a los datos\n",
        "print(\"Dimensiones del dataset:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KtUgx_nBElU"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset desde una URL p√∫blica\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "# Los nombres de las columnas se encuentran en la documentaci√≥n del dataset\n",
        "columns = ['Estado de la cuenta corriente existente', 'Duraci√≥n en meses', 'Historia de cr√©dito', 'Prop√≥sito',\n",
        "           'Monto del cr√©dito',\n",
        "           'Cuenta de ahorros/bonos', 'Empleo actual desde',\n",
        "           'Tasa de pago a plazos en porcentaje del ingreso disponible',\n",
        "           'Estado personal y sexo', 'Otros deudores/fiadores', 'Residencia actual desde', 'Propiedad',\n",
        "           'Edad', 'Otros planes de pago', 'Alojamiento', 'N√∫mero de cr√©ditos existentes en este banco',\n",
        "           'Trabajo', 'N√∫mero de personas obligadas a prestar manutenci√≥n a', 'Tel√©fono', 'Trabajador extranjero',\n",
        "           'Riesgo']\n",
        "\n",
        "df = pd.read_csv(url, sep=' ', header=None, names=columns)\n",
        "\n",
        "# Vistazo inicial a los datos\n",
        "print(\"Dimensiones del dataset:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Definici√≥n de los Diccionarios de Mapeo ---\n",
        "\n",
        "# Atributo 1: Estado de la cuenta corriente existente\n",
        "estado_cuenta_corriente_map = {\n",
        "    'A11': 'Menos de 0 DM',\n",
        "    'A12': 'Entre 0 y 200 DM',\n",
        "    'A13': 'M√°s de 200 DM / Salario asignado',\n",
        "    'A14': 'Sin cuenta corriente'\n",
        "}\n",
        "\n",
        "# Atributo 3: Historial crediticio\n",
        "historial_crediticio_map = {\n",
        "    'A30': 'Sin cr√©ditos / Todos pagados',\n",
        "    'A31': 'Todos los cr√©ditos en este banco pagados',\n",
        "    'A32': 'Cr√©ditos existentes pagados hasta ahora',\n",
        "    'A33': 'Retraso en pagos en el pasado',\n",
        "    'A34': 'Cuenta cr√≠tica / Otros cr√©ditos'\n",
        "}\n",
        "\n",
        "# Atributo 4: Prop√≥sito del cr√©dito\n",
        "proposito_map = {\n",
        "    'A40': 'Autom√≥vil (nuevo)',\n",
        "    'A41': 'Autom√≥vil (usado)',\n",
        "    'A42': 'Muebles / Equipo',\n",
        "    'A43': 'Radio / Televisi√≥n',\n",
        "    'A44': 'Electrodom√©sticos',\n",
        "    'A45': 'Reparaciones',\n",
        "    'A46': 'Educaci√≥n',\n",
        "    'A48': 'Reciclaje',\n",
        "    'A49': 'Negocios',\n",
        "    'A410': 'Otros'\n",
        "}\n",
        "\n",
        "# Atributo 6: Cuenta de ahorros/bonos\n",
        "cuenta_ahorros_map = {\n",
        "    'A61': 'Menos de 100 DM',\n",
        "    'A62': 'Entre 100 y 500 DM',\n",
        "    'A63': 'Entre 500 y 1000 DM',\n",
        "    'A64': 'M√°s de 1000 DM',\n",
        "    'A65': 'Desconocido / Sin cuenta'\n",
        "}\n",
        "\n",
        "# Atributo 7: Empleo actual desde\n",
        "empleo_actual_map = {\n",
        "    'A71': 'Desempleado',\n",
        "    'A72': 'Menos de 1 a√±o',\n",
        "    'A73': 'Entre 1 y 4 a√±os',\n",
        "    'A74': 'Entre 4 y 7 a√±os',\n",
        "    'A75': 'M√°s de 7 a√±os'\n",
        "}\n",
        "\n",
        "# Atributo 9: Estado civil y sexo\n",
        "estado_civil_sexo_map = {\n",
        "    'A91': 'Hombre: divorciado/separado',\n",
        "    'A92': 'Mujer: divorciada/separada/casada',\n",
        "    'A93': 'Hombre: soltero',\n",
        "    'A94': 'Hombre: casado/viudo',\n",
        "    'A95': 'Mujer: soltera'\n",
        "}\n",
        "\n",
        "# Atributo 10: Otros deudores/fiadores\n",
        "otros_deudores_map = {\n",
        "    'A101': 'Ninguno',\n",
        "    'A102': 'Co-solicitante',\n",
        "    'A103': 'Garante'\n",
        "}\n",
        "\n",
        "# Atributo 12: Propiedad\n",
        "propiedad_map = {\n",
        "    'A121': 'Bienes inmuebles',\n",
        "    'A122': 'Seguro de vida / Ahorro para vivienda',\n",
        "    'A123': 'Autom√≥vil u otro',\n",
        "    'A124': 'Desconocido / Sin propiedad'\n",
        "}\n",
        "\n",
        "# Atributo 14: Otros planes de pago\n",
        "otros_planes_pago_map = {\n",
        "    'A141': 'Banco',\n",
        "    'A142': 'Tiendas',\n",
        "    'A143': 'Ninguno'\n",
        "}\n",
        "\n",
        "# Atributo 15: Alojamiento\n",
        "vivienda_map = {\n",
        "    'A151': 'Alquiler',\n",
        "    'A152': 'Propia',\n",
        "    'A153': 'Gratuita'\n",
        "}\n",
        "\n",
        "# Atributo 17: Trabajo\n",
        "empleo_map = {\n",
        "    'A171': 'Desempleado / No cualificado - no residente',\n",
        "    'A172': 'No cualificado - residente',\n",
        "    'A173': 'Empleado cualificado / Funcionario',\n",
        "    'A174': 'Directivo / Aut√≥nomo / Altamente cualificado'\n",
        "}\n",
        "\n",
        "# Atributo 19: Tel√©fono\n",
        "telefono_map = {\n",
        "    'A191': 'No tiene',\n",
        "    'A192': 'S√≠, a su nombre'\n",
        "}\n",
        "\n",
        "# Atributo 20: trabajador extranjero\n",
        "trabajador_extranjero_map = {\n",
        "    'A201': 'S√≠',\n",
        "    'A202': 'No'\n",
        "}\n",
        "\n",
        "# --- 2. Aplicaci√≥n de los Mapeos al DataFrame ---\n",
        "# Se aplica el mapeo a cada columna usando los nombres en espa√±ol\n",
        "df['Estado de la cuenta corriente existente'] = df['Estado de la cuenta corriente existente'].map(estado_cuenta_corriente_map)\n",
        "df['Historia de cr√©dito'] = df['Historia de cr√©dito'].map(historial_crediticio_map)\n",
        "df['Prop√≥sito'] = df['Prop√≥sito'].map(proposito_map)\n",
        "df['Cuenta de ahorros/bonos'] = df['Cuenta de ahorros/bonos'].map(cuenta_ahorros_map)\n",
        "df['Empleo actual desde'] = df['Empleo actual desde'].map(empleo_actual_map)\n",
        "df['Estado personal y sexo'] = df['Estado personal y sexo'].map(estado_civil_sexo_map)\n",
        "df['Otros deudores/fiadores'] = df['Otros deudores/fiadores'].map(otros_deudores_map)\n",
        "df['Propiedad'] = df['Propiedad'].map(propiedad_map)\n",
        "df['Otros planes de pago'] = df['Otros planes de pago'].map(otros_planes_pago_map)\n",
        "df['Alojamiento'] = df['Alojamiento'].map(vivienda_map)\n",
        "df['Trabajo'] = df['Trabajo'].map(empleo_map)\n",
        "df['Tel√©fono'] = df['Tel√©fono'].map(telefono_map)\n",
        "df['Trabajador extranjero'] = df['Trabajador extranjero'].map(trabajador_extranjero_map)\n",
        "\n",
        "# --- 3. Verificaci√≥n de los Resultados ---\n",
        "print(\"DataFrame con valores reemplazados y columnas en espa√±ol:\")\n",
        "display(df.head(3))"
      ],
      "metadata": {
        "id": "FE4xGFsLf7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "a3UlGSKPNB4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET_ACjwuEb8G"
      },
      "source": [
        "La variable objetivo 'riesgo' est√° codificada como 1 (Bueno) y 2 (Malo). La convertiremos a 0 (Bueno) y 1 (Malo) para que la clase \"positiva\" sea el evento de inter√©s (el impago).\n",
        "\n",
        "- **0 = Buen pagador**\n",
        "- **1 = Mal pagador**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EBbdMIAEb8H"
      },
      "outputs": [],
      "source": [
        "# Usamos .map() para la recodificaci√≥n\n",
        "df['Riesgo'] = df['Riesgo'].map({1: 0, 2: 1})\n",
        "\n",
        "print(\"Distribuci√≥n de la variable objetivo 'Riesgo':\")\n",
        "print(df['Riesgo'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos un desbalance: 70% de los clientes son buenos pagadores y 30% son malos pagadores. Este **desbalance de clases**, aunque no es extremo, ya nos indica que la simple precisi√≥n (accuracy) no es la mejor m√©trica."
      ],
      "metadata": {
        "id": "y9Kxaiy0Eb8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "Vamos a convertir variables categ√≥ricas en num√©ricas usando One-Hot Encoding"
      ],
      "metadata": {
        "id": "snYWS_ahhR6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 1: Separar X e y ---\n",
        "# X contiene TODAS las variables predictoras, tanto num√©ricas como categ√≥ricas\n",
        "X = df.drop('Riesgo', axis=1)\n",
        "y = df['Riesgo']\n",
        "\n",
        "# --- PASO 2: Divisi√≥n en train y test ---\n",
        "# Esto es crucial para evitar data leakage. El encoder solo debe \"ver\" los datos de entrenamiento.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(\"Forma de X_train original:\", X_train.shape)\n",
        "print(\"Forma de X_test original:\", X_test.shape)\n",
        "\n",
        "# --- PASO 3: Identificar columnas y crear el preprocesador ---\n",
        "\n",
        "# Identificamos autom√°ticamente las columnas num√©ricas y categ√≥ricas desde X_train\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Crear el transformador para las variables categ√≥ricas\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
        "\n",
        "# Crear el ColumnTransformer\n",
        "# 'passthrough' indica que las columnas num√©ricas no deben ser modificadas\n",
        "# En √°rboles de decisi√≥n, random forest o boosting no es necesario estandarizar las variables num√©ricas,\n",
        "# pero hacerlo, tampoco es perjudicial\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('num', 'passthrough', numerical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Por si alguna columna no fue listada, se le indica que no le haga nada\n",
        ")\n",
        "\n",
        "# --- PASO 4: Aplicar el preprocesador ---\n",
        "\n",
        "# \"Aprender\" (fit) las categor√≠as SOLO de X_train y transformar X_train\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Usar el preprocesador ya \"aprendido\" para transformar X_test\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# --- Verificaci√≥n de resultados ---\n",
        "print(\"\\nForma de X_train procesado:\", X_train.shape)\n",
        "print(\"Forma de X_test procesado:\", X_test.shape)"
      ],
      "metadata": {
        "id": "RWJcKkGSE0Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YKVFDfaT8Ui"
      },
      "source": [
        "## 2. Modelo Base: Un Solo √Årbol de Decisi√≥n\n",
        "\n",
        "Antes de construir un \"bosque\", entendamos el rendimiento de un solo \"√°rbol\". Este ser√° nuestro punto de comparaci√≥n para ver si la complejidad de los ensambles realmente aporta valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbS0cCm5T8Ui"
      },
      "outputs": [],
      "source": [
        "# Inicializar y entrenar el √Årbol de Decisi√≥n\n",
        "tree_model = DecisionTreeClassifier(random_state=42, max_depth=20) # Limitamos la profundidad para evitar sobreajuste extremo\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "\n",
        "# Visualizar la matriz de confusi√≥n\n",
        "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
        "sns.heatmap(cm_tree, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicci√≥n 0 (Bueno)', 'Predicci√≥n 1 (Malo)'],\n",
        "            yticklabels=['Real 0 (Bueno)', 'Real 1 (Malo)'])\n",
        "plt.ylabel('Valor Real')\n",
        "plt.xlabel('Valor Predicho')\n",
        "plt.title('Matriz de Confusi√≥n √Årbol de Decisi√≥n')\n",
        "plt.show()\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(\"Evaluaci√≥n del √Årbol de Decisi√≥n:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tree):.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1X6dIzPT8Uj"
      },
      "source": [
        "**Interpretaci√≥n:**\n",
        "\n",
        "El √°rbol √∫nico tiene una `exactitud` del 64%. Sin embargo, miremos m√°s de cerca el `recall` para la clase 1 (mal pagador): es solo de 0.42. Esto significa que **nuestro modelo solo est√° identificando al 42% de los clientes que realmente van a incumplir**. Para un banco, ¬°esto es muy riesgoso! Estamos dejando pasar a m√°s de la mitad de los malos clientes.\n",
        "\n",
        "La `precisi√≥n` tambi√©n es baja. De los que el modelo predijo que eran malos pagadores, solo 40% son realmente malos pagadores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edh2w7gMT8Uj"
      },
      "source": [
        "## 3. Random Forest: La Sabidur√≠a de la Multitud üå≥üå≥üå≥"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasta ahora, hemos usado un solo √Årbol de Decisi√≥n. Siguiendo nuestra analog√≠a, esto es como confiar en el juicio de un √∫nico \"analista estrella\". ¬øPero qu√© pasa si ese analista tiene un sesgo particular o se ha sobreajustado a una mala racha del mercado? Su modelo ser√° inestable y poco fiable.\n",
        "\n",
        "**Random Forest (Bosque Aleatorio)** propone una soluci√≥n mucho m√°s robusta, basada en el principio de \"la sabidur√≠a de la multitud\".\n",
        "\n",
        "En lugar de confiar en un solo analista, vamos a crear un comit√© de inversi√≥n diverso con, por ejemplo, 300 analistas (√°rboles). La decisi√≥n final no la tomar√° uno solo, sino que ser√° el promedio o la votaci√≥n de todo el comit√©.\n",
        "\n",
        "Para que este comit√© funcione, necesitamos dos cosas:\n",
        "\n",
        "- Competencia: Cada analista (√°rbol) debe ser razonablemente bueno (mejor que adivinar al azar).\n",
        "\n",
        "- Diversidad: Los analistas no deben cometer los mismos errores. Sus opiniones deben ser no correlacionadas. Si todos piensan igual, el comit√© es in√∫til.\n",
        "\n",
        "Random Forest logra esta diversidad de forma brillante usando dos t√©cnicas de aleatorizaci√≥n:\n",
        "\n",
        "1. **Aleatorizaci√≥n de Datos (Bagging)**\n",
        "\n",
        "No le damos a todos los analistas exactamente los mismos datos hist√≥ricos. En lugar de eso, a cada uno de los 300 √°rboles le damos una muestra aleatoria del conjunto de entrenamiento.\n",
        "\n",
        "Esta t√©cnica se llama Bagging. Para un dataset de 800 clientes de entrenamiento, cada √°rbol recibir√° una muestra de 800 clientes elegidos con reemplazo. Esto significa que la muestra de cada √°rbol ser√° ligeramente diferente: algunas contendr√°n clientes repetidos y otras omitir√°n algunos clientes por completo.\n",
        "\n",
        "Esto asegura que cada √°rbol tenga una \"visi√≥n\" del mundo ligeramente distinta, evitando que todos aprendan exactamente los mismos patrones.\n",
        "\n",
        "2. **Aleatorizaci√≥n de Variables (Features)**\n",
        "\n",
        "Si hay una variable muy predictiva (ej. historial_credito), todos los √°rboles tender√≠an a usarla como su primera pregunta. Esto har√≠a que todos los √°rboles fueran muy similares y sus errores estar√≠an correlacionados.\n",
        "\n",
        "Para evitar esto, Random Forest a√±ade una segunda capa de aleatoriedad: en cada paso de divisi√≥n de un √°rbol, no le permite ver todas las variables. Solo le da un subconjunto aleatorio de ellas (ej. \"solo puedes elegir entre 'edad', 'prop√≥sito' y 'ahorros' para esta decisi√≥n\").\n",
        "\n",
        "Esto fuerza a los √°rboles a explorar otras variables y encontrar patrones diferentes. Un √°rbol podr√≠a descubrir un patr√≥n en la 'edad' y otro en el 'monto_credito', haciendo que el comit√© en su conjunto sea mucho m√°s diverso y sabio.\n",
        "\n",
        "**¬øC√≥mo Funciona el Modelo?**\n",
        "\n",
        "- Se construyen n_estimators (ej. 300 √°rboles de decisi√≥n).\n",
        "\n",
        "  - Cada √°rbol se entrena con una muestra (Bagging).\n",
        "  - En cada divisi√≥n de cada √°rbol, solo se considera un subconjunto aleatorio de max_features (variables).\n",
        "\n",
        "- Predicci√≥n (Votaci√≥n):\n",
        "\n",
        "  - Para un nuevo cliente, se le pasa su informaci√≥n a los 300 √°rboles.\n",
        "  - Cada √°rbol da su predicci√≥n (ej. 70 √°rboles votan \"Riesgo 0\" y 30 √°rboles votan \"Riesgo 1\").\n",
        "  - La predicci√≥n final del bosque es la clase m√°s votada (\"Riesgo 0\"). Si es un problema de regresi√≥n, entonces la predicci√≥n final corresponde al promedio.\n",
        "\n",
        "**La ventaja principal del Random Forest es que reduce el sobreajuste (overfitting).** Los errores de un √°rbol individual (que se sobreajusta) se promedian y cancelan con los errores de los dem√°s. El \"promedio del comit√©\" es mucho m√°s estable que un solo √°rbol de decisi√≥n.\n"
      ],
      "metadata": {
        "id": "w9SD_grZs1-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, en lugar de un solo √°rbol, vamos a crear un bosque de 300 √°rboles."
      ],
      "metadata": {
        "id": "OoP3K4has54P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYU2j3ctT8Uj"
      },
      "outputs": [],
      "source": [
        "# Inicializar y entrenar el Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=300, random_state=42, max_depth=20, n_jobs=-1) # n_jobs=-1 usa todos los procesadores del computador\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Visualizar la matriz de confusi√≥n\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicci√≥n 0 (Bueno)', 'Predicci√≥n 1 (Malo)'],\n",
        "            yticklabels=['Real 0 (Bueno)', 'Real 1 (Malo)'])\n",
        "plt.ylabel('Valor Real')\n",
        "plt.xlabel('Valor Predicho')\n",
        "plt.title('Matriz de Confusi√≥n Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(\"Evaluaci√≥n del Random Forest:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_0GWliT8Uj"
      },
      "source": [
        "**Interpretaci√≥n:** ¬°Mejoramos! La `exactitud` subi√≥ al 78%. M√°s importante a√∫n, el `recall` para la clase 1 (mal pagador) mejor√≥ a 45% y la `precisi√≥n` subi√≥ de forma importante a 71%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCAEhoLT8Uk"
      },
      "source": [
        "## 4. Gradient Boosting: Aprendiendo de los Errores üë®‚Äçüè´"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si Random Forest es un \"comit√© democr√°tico\" que vota en paralelo, Gradient Boosting es un \"programa de mentor√≠a\" jer√°rquico que aprende de forma secuencial.\n",
        "\n",
        "La intuici√≥n central de Gradient Boosting es aprender de los errores. En lugar de construir 300 modelos independientes, construimos un modelo a la vez, donde cada nuevo modelo se dedica a corregir los errores que cometi√≥ el anterior.\n",
        "\n",
        "1. Se entrena un primer √°rbol de decisi√≥n simple (ej. max_depth=3). Este √°rbol hace una predicci√≥n inicial sobre la variable objetivo (ser buen o mal pagador en este caso).\n",
        "\n",
        "2. Se calcula el error que este √°rbol comete para cada cliente (Error1 = Dato Real - Predicci√≥n(√Årbol 1)).\n",
        "\n",
        "3. Se entrena un segundo √°rbol, cuya variable objetivo es el error que cometi√≥ el √Årbol 1 (Error1). Las variables (features) son las mismas del modelo original (Edad, Deuda, ...).\n",
        "\n",
        "4. Se calcula una nueva predicci√≥n: Predicci√≥n combinada = Predicci√≥n(√Årbol 1) + Predicci√≥n(√Årbol 2). De esta forma, la predicci√≥n se va acercando m√°s al valor real.\n",
        "\n",
        "5. Se calculan los errores de la predicci√≥n combinada (Error2 = Dato Real - Predicci√≥n Combinada).\n",
        "\n",
        "6. Se entrena un tercer √°rbol cuya variable objetivo es el error de la predicci√≥n combinada (Error2).\n",
        "\n",
        "Este proceso se repite 'n_estimators' veces. Cada nuevo √°rbol se enfoca en los casos m√°s dif√≠ciles que el \"comit√©\" anterior no pudo resolver, refinando la predicci√≥n paso a paso hasta que los errores son m√≠nimos.\n",
        "\n",
        "El Hiperpar√°metro Clave:\n",
        "\n",
        "**Tasa de Aprendizaje (learning_rate):** controla la magnitud de la correcci√≥n.\n",
        "\n",
        "En lugar de sumar la predicci√≥n completa:\n",
        "\n",
        "Predicci√≥n combinada = Predicci√≥n(√Årbol 1) + Predicci√≥n(√Årbol 2)\n",
        "\n",
        "Se hace una correcci√≥n m√°s cautelosa:\n",
        "\n",
        "Predicci√≥n combinada = Predicci√≥n(√Årbol 1) + learning_rate * Predicci√≥n(√Årbol 2)\n",
        "\n",
        "Un 'learning_rate' bajo significa que cada √°rbol hace una correcci√≥n peque√±a y sutil. Esto evita que el modelo corrija en exceso (sobreajuste) y le permite encontrar un resultado √≥ptimo de forma m√°s estable.\n",
        "\n",
        "**La principal ventaja de Gradient Boosting es que al enfocarse secuencialmente en los errores, puede alcanzar un rendimiento predictivo extremadamente alto.** Suele ser uno de los mejores algoritmos para datos tabulares (como los que usamos en finanzas y econom√≠a).\n",
        "\n",
        "La implementaci√≥n m√°s famosa y exitosa de este m√©todo es **XGBoost (Extreme Gradient Boosting)**, conocida por su eficiencia y por ganar innumerables competencias de ciencia de datos.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "- Requiere un ajuste cuidadoso de hiperpar√°metros para evitar el sobreajuste.\n",
        "\n",
        "- A diferencia de Random Forest, no se puede paralelizar (n_jobs=-1 no sirve de mucho), ya que cada √°rbol depende del resultado del anterior.\n",
        "\n"
      ],
      "metadata": {
        "id": "3S22R50tuzCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El √°rbol de decisi√≥n y el random forest ten√≠an una profundidad de 20, sin embargo, en Gradient Boosting se utilizan modelos sencillos (llamados 'd√©biles'), que son poco profundos, as√≠ que vamos a utilizar una profundidad de 3. Adicionalmente, vamos a decirle que construya 100 √°rboles (o que itere 100 veces)."
      ],
      "metadata": {
        "id": "38Lx5om9CbrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmlLFQdHT8Uk"
      },
      "outputs": [],
      "source": [
        "# Inicializar y entrenar el Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Visualizar la matriz de confusi√≥n\n",
        "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
        "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicci√≥n 0 (Bueno)', 'Predicci√≥n 1 (Malo)'],\n",
        "            yticklabels=['Real 0 (Bueno)', 'Real 1 (Malo)'])\n",
        "plt.ylabel('Valor Real')\n",
        "plt.xlabel('Valor Predicho')\n",
        "plt.title('Matriz de Confusi√≥n √Årbol de Decisi√≥n')\n",
        "plt.show()\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(\"Evaluaci√≥n de Gradient Boosting:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfQCj2DT8Uk"
      },
      "source": [
        "**Interpretaci√≥n:** Los resultados son a√∫n mejores. La `exactitud` es del 80% y el `recall` para la clase 1 (mal riesgo) subi√≥ a **0.57**. ¬°Ahora estamos identificando al 57% de los malos clientes! La `precisi√≥n` baj√≥ de 0.71 a 0.69, pero el f1-score subi√≥ de 0,7 a 0.74."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bbCKrirT8Ul"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o\n",
        "\n",
        "Como vimos, la exactitud (accuracy) no nos cuenta toda la historia, especialmente con clases desbalanceadas. Un banco podr√≠a preferir un modelo que identifique a m√°s clientes de alto riesgo (mayor `recall`), incluso si eso significa clasificar err√≥neamente a algunos buenos clientes como malos (menor `precision`).\n",
        "\n",
        "La **Curva ROC (Receiver Operating Characteristic)** nos ayuda a visualizar este trade-off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQFkLNWgT8Ul"
      },
      "source": [
        "**¬øQu√© es la Curva ROC?**\n",
        "\n",
        "* **Eje Y (Tasa de Verdaderos Positivos / Recall):** ¬øQu√© proporci√≥n de los malos clientes reales identificamos correctamente?\n",
        "* **Eje X (Tasa de Falsos Positivos):** ¬øQu√© proporci√≥n de los buenos clientes identificamos incorrectamente como malos?\n",
        "\n",
        "Un modelo ideal estar√≠a en la esquina superior izquierda (100% de Verdaderos Positivos, 0% de Falsos Positivos). La l√≠nea diagonal representa un modelo que adivina al azar. Queremos que nuestra curva est√© lo m√°s lejos posible de esa l√≠nea.\n",
        "\n",
        "El **√Årea Bajo la Curva (AUC)** resume esta gr√°fica en un solo n√∫mero. Un AUC de 0.5 es azar y un AUC de 1.0 es un modelo perfecto. Mide la habilidad del modelo para distinguir entre un cliente bueno y uno malo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEH_WkfTT8Ul"
      },
      "outputs": [],
      "source": [
        "# Obtener las probabilidades de predicci√≥n para la clase positiva (riesgo=1)\n",
        "y_prob_tree = tree_model.predict_proba(X_test)[:, 1]\n",
        "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "y_prob_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular FPR, TPR y AUC para cada modelo\n",
        "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_prob_tree)\n",
        "auc_tree = auc(fpr_tree, tpr_tree)\n",
        "\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
        "auc_rf = auc(fpr_rf, tpr_rf)\n",
        "\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_gb)\n",
        "auc_gb = auc(fpr_gb, tpr_gb)\n",
        "\n",
        "# Graficar las Curvas ROC\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_tree, tpr_tree, label=f'√Årbol de Decisi√≥n (AUC = {auc_tree:.3f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.3f})')\n",
        "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {auc_gb:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Clasificador Aleatorio') # L√≠nea de azar\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (Recall)')\n",
        "plt.title('Curvas ROC para Modelos de Riesgo Crediticio')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhT2RDlT8Ul"
      },
      "source": [
        "**Interpretaci√≥n de las Curvas ROC y AUC:**\n",
        "\n",
        "El modelo de **Random Forest (AUC=0.796)** es mejor que el **Gradient Boosting (AUC=0.775)** y mucho mejor que el **√Årbol de Decisi√≥n (AUC=0.576)** para la tarea de *distinguir* entre clientes buenos y malos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Del An√°lisis ROC a la Decisi√≥n de Negocio: Ajustando el Umbral"
      ],
      "metadata": {
        "id": "dRxNiINZwoOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya hemos encontrado el modelo con mejor desempe√±o para nuestro caso: el Random Forest. Pero el paso final para que un modelo sea verdaderamente √∫til en un contexto econ√≥mico es alinear sus predicciones con un objetivo de negocio.\n",
        "\n",
        "Por defecto, un modelo de clasificaci√≥n usa un umbral de 0.5 para decidir entre \"Riesgo 0\" y \"Riesgo 1\". Este umbral es arbitrario y casi nunca es el ideal.\n",
        "\n",
        "Para nuestro banco, el costo de no detectar a un cliente de \"mal riesgo\" (un Falso Negativo) es mucho m√°s alto que el costo de verificar dos veces a un cliente bueno (un Falso Positivo).\n",
        "\n",
        "Por lo tanto, no buscamos el umbral que da el mejor accuracy, sino el que cumple nuestro objetivo de negocio: **Capturar al menos el 80% de los clientes de mal riesgo (Recall = 0.8)**\n",
        "\n",
        "En esta secci√≥n, usaremos los datos de la Curva ROC para encontrar el umbral de probabilidad exacto que cumple este objetivo. Luego, dejaremos de usar .predict() y aplicaremos este nuevo umbral para crear nuestro sistema de decisi√≥n final."
      ],
      "metadata": {
        "id": "8BZ98HKc1Y_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Encontrar el umbral √≥ptimo para Random Forest ---\n",
        "\n",
        "# Obtener las probabilidades de predicci√≥n para la clase positiva (riesgo=1)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular la curva ROC\n",
        "# (tpr = True Positive Rate = Recall)\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)\n",
        "\n",
        "# Definir nuestro objetivo de negocio\n",
        "objetivo_recall = 0.80 # Por ejemplo, queremos un recall del 80%\n",
        "\n",
        "# Encontrar el umbral que cumple nuestro objetivo\n",
        "try:\n",
        "    indice_objetivo_rf = np.argmax(tpr_rf >= objetivo_recall)\n",
        "    umbral_elegido_rf = thresholds_rf[indice_objetivo_rf]\n",
        "    costo_fpr_rf = fpr_rf[indice_objetivo_rf]\n",
        "    recall_obtenido_rf = tpr_rf[indice_objetivo_rf]\n",
        "\n",
        "    print(f\"--- Decisi√≥n de Negocio (Random Forest) ---\")\n",
        "    print(f\"Para alcanzar un Recall de al menos {objetivo_recall*100:.0f}% (obtuvimos {recall_obtenido_rf*100:.1f}%):\")\n",
        "    print(f\"Debes usar un umbral de decisi√≥n de: {umbral_elegido_rf:.4f}\")\n",
        "    print(f\"El costo ser√° una Tasa de Falsos Positivos de: {costo_fpr_rf*100:.1f}%\\n\")\n",
        "\n",
        "except ValueError:\n",
        "    print(\"El modelo Random Forest no alcanza el recall objetivo.\")\n",
        "    umbral_elegido_rf = 0.5 # Usar default si falla\n",
        "\n",
        "\n",
        "# --- Aplicar el nuevo umbral a las predicciones ---\n",
        "\n",
        "if 'umbral_elegido_rf' in locals():\n",
        "    # Usamos el umbral que acabamos de encontrar\n",
        "    predicciones_rf_umbral = (y_prob_rf >= umbral_elegido_rf).astype(int)\n",
        "\n",
        "    # Evaluar el rendimiento con el nuevo umbral\n",
        "    print(f\"--- Reporte de Clasificaci√≥n (Umbral RF = {umbral_elegido_rf:.4f}) ---\")\n",
        "    print(classification_report(y_test, predicciones_rf_umbral))\n",
        "\n",
        "    # Visualizar la matriz de confusi√≥n\n",
        "    cm_rf_umbral = confusion_matrix(y_test, predicciones_rf_umbral)\n",
        "    sns.heatmap(cm_rf_umbral, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Predicci√≥n 0 (Bueno)', 'Predicci√≥n 1 (Malo)'],\n",
        "                yticklabels=['Real 0 (Bueno)', 'Real 1 (Malo)'])\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.xlabel('Valor Predicho')\n",
        "    plt.title('Matriz de Confusi√≥n RF con Umbral Personalizado')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "onHcbvwYwv4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto tiene un trade-off (costo-beneficio): est√°s aceptando clasificar incorrectamente a m√°s clientes buenos (m√°s Falsos Positivos, bajando el recall de la clase 0) a cambio de capturar muchos m√°s clientes malos (m√°s Verdaderos Positivos, subiendo el recall de la clase 1)."
      ],
      "metadata": {
        "id": "WuAKPPARwFd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Variables relevantes"
      ],
      "metadata": {
        "id": "wra41RmKLslx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestros modelos de ensamble son muy precisos, pero a menudo se les critica por ser \"cajas negras\". Si no podemos explicar por qu√© un modelo toma una decisi√≥n, ¬øc√≥mo podemos confiar en √©l para decisiones econ√≥micas cr√≠ticas?\n",
        "\n",
        "Afortunadamente, **los modelos basados en √°rboles nos ofrecen una poderosa herramienta para ganar interpretabilidad: la importancia de variables (atributo .feature_importances_).**\n",
        "\n",
        "Este atributo nos muestra qu√© variables fueron las m√°s consultadas y decisivas para el \"comit√©\" de √°rboles al momento de clasificar el riesgo. Para el banco, esto es muy √∫til, ya que nos dice qu√© factores (ej. status_cuenta, duracion_mes) son los verdaderos impulsores del riesgo.\n",
        "\n",
        "El siguiente c√≥digo extrae estas importancias de nuestro Random Forest, las conecta con los nombres de las variables ya procesadas (get_feature_names_out()) y grafica las 15 m√°s relevantes."
      ],
      "metadata": {
        "id": "n7wwb_Ek4Ur7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los nombres de las variables\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Obtener las importancias del modelo\n",
        "rf_importances = rf_model.feature_importances_\n",
        "\n",
        "# Crear un DataFrame para facilitar la visualizaci√≥n\n",
        "rf_importance_df = pd.DataFrame({\n",
        "    'Variable': feature_names,\n",
        "    'Importancia': rf_importances\n",
        "}).sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "# Graficar las 15 variables m√°s importantes\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    x='Importancia',\n",
        "    y='Variable',\n",
        "    data=rf_importance_df.head(15),\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title('Top 15 Variables m√°s Importantes - Random Forest', fontsize=16)\n",
        "plt.xlabel('Importancia (Gini)', fontsize=12)\n",
        "plt.ylabel('Variable', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "diJ5_TJrnaEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}